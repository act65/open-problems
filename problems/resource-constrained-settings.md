Right now deep learning only works with lots of data, lots of compute, (and lots of patience...).
Given that we are seeing DL massively scaled across the internet, small wins in efficiency can make a big deal.

#### Small compute

Can we save compute by

#### Small memory

Online algorithms.

#### Small data

?


## Big data

Machine learning is about finding patterns in data. Over the last few years datasets have been rapidly increasing in size, and for good reason. The larger the dataset, the more robust and accurate the learned algorithm. However, as these datasets increase in size we start to encounter new problems. Requiring $$$ for electricity, ...
Making ML only for the rich and powerful. We need ...

Dont want to have to train on multi epochs when the size of the dataset is large.
Want sublinear costs in; memory, compute, data,

- Mixture of experts
- Active learning


When you have a lot of data... Unsupervised/reinforcement!
