Fundamentally, learning must be distributed because compute and memory must be distributed. Computation and memory reduces to physics, and physics tells us there is a universal limit to information density and processing.
Typically, a computers architecture abstracts away the need to consider this.
However, can we be smarter in designing our system to reflect the computations done in learning.

<!-- distributed in space or time or ??  memory in space == memory in time? -->


- TPU, GPU,
- neuromorphic computing


Not just the compute needs to be distributed, but the data will already be distributed.

- Federated optimisation.


What if;
- inhomogenous compute, how can each node contribute most effectively? (related to multi-agent-systems?)
- low communication bandwidth,
-
