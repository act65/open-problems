Fundamentally, learning must be distributed. This is because; computation and storage are fundamentally distributed. Physics tells us there is a universal limit to information density.
Typically, a computers architecture abstracts away the need to consider this. However, ... we could be smarter in designing our system to reflect the computations done in learning.

<!-- distributed in space or time or ?? -->


- TPU, GPU,
- neuromorphic computing


Not just the compute needs to be distributed, but the data will already be distributed.
- Federated optimisation.


What if;
- inhomogenous compute, how can each node contribute most effectively? (seems close to multi-learner-systems?)
- low communication bandwidth,
