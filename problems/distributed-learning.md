Fundamentally, learning must be distributed because compute and memory must be distributed. (Computation and memory reduces to physics, and physics tells us there is a universal limit to information density and processing)
Typically, programming languages are designed to abstract us away from this. However, can we be smarter in designing our system to reflect the computations done in learning.

<!-- distributed in space or time or ??  memory in space == memory in time? -->


#### Efficient communication

Since we have distributed the compute, we need to collect and communicate .

Compress data before communicating (but at the lowest level possible)? 
Each node/agent/? At the lowest level, the 'code' would be rather limited? Want to be able to tailor?
??? 

#### Inhomogenous compute


Note that if compute is distributed, then data will also need to be distributed.

* [Federated optimisation](https://arxiv.org/abs/1511.03575)


What if;
- inhomogenous compute, how can each node contribute most effectively? (related to multi-agent-systems?)
- low communication bandwidth,
-
