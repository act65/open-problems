What are their downfalls? How can we make them more efficient? What is it that they are doing that is important? Answers to these questions should help us build better learners.

- Adversarial examples.
- Interpreting learned ...

Progress in unsupervised learning is likely to be made through our ability to specify domain specific priors (such as ???). This is because it is not possible to have a universally smart algol (see ???).

What we need is better, more flexible and efficient (in compute, data ???), way to specify arbitrary priors on our models.

Relatedly, what domains do we really care about and need to find better priors?

Priors can ? models in a few different ways; loss/regularisation, layers, topological structure, weight sharing, ?
