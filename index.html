<!DOCTYPE html>
<html>
<head>
    <link href="http://netdna.bootstrapcdn.com/twitter-bootstrap/2.3.0/css/bootstrap-combined.min.css" rel="stylesheet">
    <style>
        body {
            font-family: sans-serif;
        }
        code, pre {
            font-family: monospace;
            }
            h1 code,
            h2 code,
            h3 code,
            h4 code,
            h5 code,
            h6 code {
                font-size: inherit;
        }
        div {
            margin-right: 400px;
            margin-left: 400px;
        }
    </style>
</head>
<body>
<div class="container">
<h1>  better-priors</h1>

<p>What are their downfalls? How can we make them more efficient? What is it that they are doing that is important? Answers to these questions should help us build better learners.</p>
<ul>
<li>Adversarial examples.</li>
<li>Interpreting learned &hellip;</li>
</ul>
<p>Progress in unsupervised learning is likely to be made through our ability to specify domain specific priors (such as ???). This is because it is not possible to have a universally smart algol (see ???).</p>
<p>What we need is better, more flexible and efficient (in compute, data ???), way to specify arbitrary priors on our models.</p>
<p>Relatedly, what domains do we really care about and need to find better priors?</p>
<p>Priors can ? models in a few different ways; loss/regularisation, layers, topological structure, weight sharing, ?</p>
<p><hr><h1>  multi-agent-systems</h1>Why is this important?
We want modular learning system, this means we need multiple loss fns with different learners.
But how should learners interact to minimize a loss. What happens if some learner want to maximise the loss as well.</p>
<ul>
<li>Dynamical systems</li>
<li>Mechanism design</li>
<li>Mutli-objective optimisation</li>
<li>Type systems(?)</li>
<li>Modular and first class networks
<!-- how does this relate to reasoning??? --></li>
</ul>
<!--
i think there might be a couple separate problems here
- stability in optimising multiple losses
- manipulating modules
- sharing? transfer? ??
-
-->

<p>Ensembles?</p>
<p>Related to learning by curriculum and transfer learning. Problems can often be decomposed into a set of smaller sub problems</p>
<ul>
<li>Training networks with networks. Distillation, multi-modal target matching, eyes teaching proprioception.</li>
</ul>
<p><hr><h1>  disentangled-features</h1>
<hr><h1>  learning-complex-functions</h1>Most real world problems are a little more complicated than differentiating between pictures of cats and dogs. And as such the require far more knowledge (/intelligence).
How can we learn complex functions (without having to &hellip; resources)</p>
<ul>
<li>Curriculum, (two types? target and capacity?)</li>
<li>
<h2>long-term and deep,</h2>
</li>
</ul>
<p>What do we mean by complicated?</p>
<p>Constructing more complex fns from simple ones. Starting with the simplest hypotheses.</p>
<p><hr><h1>  distributed-learning</h1>Fundamentally, learning must be distributed because compute and memory must be distributed. (Computation and memory reduces to physics, and physics tells us there is a universal limit to information density and processing)
Typically, programming languages are designed to abstract us away from this. However, can we be smarter in designing our system to reflect the computations done in learning.</p>
<!-- distributed in space or time or ??  memory in space == memory in time? -->

<h4>Efficient communication</h4>
<p>Since we have distributed the compute, we need to collect and communicate .</p>
<p>Compress data before communicating (but at the lowest level possible)? 
Each node/agent/? At the lowest level, the &lsquo;code&rsquo; would be rather limited? Want to be able to tailor?
??? </p>
<h4>Inhomogenous compute</h4>
<p>Note that if compute is distributed, then data will also need to be distributed.</p>
<ul>
<li><a href="https://arxiv.org/abs/1511.03575">Federated optimisation</a></li>
</ul>
<p>What if;
- inhomogenous compute, how can each node contribute most effectively? (related to multi-agent-systems?)
- low communication bandwidth,
-</p>
<p><hr><h1>  graph-representations</h1>In deep learning all information is represented in finite dimensional vectors spaces, arrays. Can we generalise this to graphs (where an array is just
 a uniform grad graph)?</p>
<p>Why bother? Graphs allow us to represent relations, between nodes. An extra layer of information.</p>
<h4>Composition</h4>
<p>If we have two graphs representing idea <strong>1</strong> and <strong>2</strong>, then how can we efficiently compose these to integrate their knowledge?
Trivially, we could just join where they share similar nodes? But what if we are working with nodes embedded in some real space, where it is non trivial to check similarity? Or, ?</p>
<h4>Compression</h4>
<p>What about compressing a graph?</p>
<h4>Hierarchical</h4>
<p>Sketching a graph with ? (a fixed length vector?) so we can </p>
<p>Hierarchical graphs. Graphs within graphs, where a node is a graph, and the edge is ?.</p>
<h4>Construction</h4>
<p>How can leared systems interact with, output, read from, &hellip; graph structures. Adjacency matrix feels hack. Want to build into computational primitives, need better libraries?</p>
<p>Graph signal processing?</p>
<p><hr><h1>  annotated-datasets</h1>Supervised learning requires annotated data!
This means: data is where the value is at. So, what incentives are there to open data to the public? None. Unless, ?.</p>
<p>Possible (half) solution?
A cheaper way to annotate data. Hierarchical label structures? Interactive exploration of data (that is also fun)?</p>
<p>New and clever ways of extracting annotations from existing datasets. Something like the how unsupervised methods extract labels from the existing data. Train on context, recontruction, ?, real/fake, &hellip;</p>
<p>Projects that satisfy these wants;
<em> Interactive tensorboard visualisations. Ability to add meta data while viewing the <a href="http://projector.tensorflow.org/">projector</a>.
</em> ?</p>
<p><hr><h1>  scaling-deep-learining</h1>Deep learning is really a meta algorithm. Take a set of differentiable layers, compose them together into a hierarchy, use automatic differentiation to efficiently compute gradients w.r.t each layer and stochastic gradient descent to update each layer.
Long story short, it works (as long as you have a lot of labelled data).</p>
<p>So let&rsquo;s apply this approach to as many domains as possible. But, there are some functions that are not differentiable,</p>
<ul>
<li>random variables</li>
<li>discrete variables</li>
<li>trees (diff boundary trees), graphs</li>
<li>unknown (ie RL setting)</li>
</ul>
<!-- Examples; MuProp, ?

differentiable indexing.
differentiable sparse gating -->

<p><hr><h1>  specifying-goals</h1>Insert pic of lion.</p>
<p>How do you determine if the image you are looking at is a lion?
Ok, write that process down as a program. Oh&hellip; That&rsquo;s really hard.</p>
<p>If we can write what we want down as a clear function/equation, then we can probably optimise it. (how?) If not. Hmm.</p>
<p>Machine learning allows an alternative approach to specifying what we want. It allows us to &lsquo;show&rsquo; the computer what we want by example. Pairs for images and their labels, &hellip;</p>
<p>It is not always possible to use examples to show the computer what we want, for examples learning p(x). GANs? Allow us to extend this &hellip;
<!-- What about generators of goals? (that RL stuff?) --></p>
<p>In RL our goal is some game state/set of states. It seems reasonable to be able to compress this set of game states into some sort of hidden representation that we can compare against. Or communicate to others.
<!-- But if you set the goal as a game state, then how does improvement make sense? Once the state has been achieved there is nothing left to do...? --></p>
<p>However. What about when the goal is some process or function or algorithm? How can we represent sets of these? By their input/output? &hellip;??</p>
<!-- This is closely related to learning loss functions! -->

<p>Different ways to specify a goal/function:</p>
<!-- What do we mean by specify?
- Choose,
- narrow down,
-

so it is a way to reduce search space.
what about falsification?
-->

<ul>
<li>can show by example, (but what if we only know the goal, and not how that relates to inputs? that is just RL, as opposed to SL, where we have pairs.)</li>
<li>have a function, f, that tells us (true, false, &hellip;) whether y is the goal (where f may or may not be differentiable). UL or SL.</li>
<li>Reducing possible options. eg. can only pick from some set of functions (e.g. linear, &hellip;).</li>
<li>randomly&hellip;</li>
</ul>
<p>What about if we dont know what we want? Then we must either;</p>
<ul>
<li>specify a process for finding new things,</li>
<li>simulate all possible goals and then pick out the ones we like.</li>
</ul>
<p>What if we could specify goals using natural language.</p>
<p>What about the efficient specification of goals? Examples take space, equations do not.</p>
<p>More abstractly, how can two agents (that work differently) communicate their goals/processes?</p>
<p><hr><h1>  real-time-prediction</h1>Humans evolved in a world where we needed to make predictions quickly, given little information and resources. Main problems;</p>
<h4>Aligning delayed signals/rewards</h4>
<p>Image that the vision system might process info within 0.1ms, but the auditory system processed it in 0.5ms. How can we align these two pieces of information, to build a full model of our environment?
And, given that there was a delay in when; we percived, and acted, recieved (/percieved) a reward how can we assign credit to act single choice we made in between?</p>
<h4>Shared resources</h4>
<p>Imagine a system that has to learn and predict with the same set of minimal resources.
An obvious strategy might be to: predict now, consolidate/learn later? Which seems familiar?</p>
<h4>Complex functions</h4>
<p>Processing complex functions cause a delay in response. A way around this is to attempt to predict the future and your next step (do the computation early rather than reactively).
This also allows correction/response to happen faster, as we dont need to percieve what is going on (if we are right), we can just focus on where we predicted wrong and allocate attention/compute there.</p>
<p><hr><h1>  effective-explanations</h1>Deep learning needs to see the same dataset many times, eventually learns some useful patterns, but not before wandering here and there to checkout what it would be like to do a crappy job.</p>
<h4>One-shot</h4>
<p>We (seem to ) have the ability to learn from a single example. How is this possible? Can we get computers to do it?</p>
<p>Trivially, this is just memorisation. The interesting part comes from trying to achieve a sub-linear memory footprint, aka some sort of online compression. And this is where we get back to deep learning. We need to pick some method to compress these examples we are shooting, and deep learning seems like a decent canidate, except for its data complexity.</p>
<p>Most definitions of learning (e.g. PAC) include some garuntee that a single example should not change the output of the learning algorithm by more than epsilon. This seems to preclude the ability to do one-shot learning?</p>
<!-- How do current methods get around this? -->

<!-- Could be closer to memory, decompose new input into things already known, memorise signal over them. Then learning would be the how to decompose. Aka one-shot == memory. -->

<h4>Zero-shot</h4>
<p>If one-shot learning is: &ldquo;Learning from a single example&rdquo; then zero-shot learning would be: &ldquo;making a damn good guess&rdquo;.</p>
<p>Given examples about X, can we learn about Y?</p>
<p>A possible hypothesis for how we manage this is that we hypothesise about causes, we create models/explanations, which are occasionally accurate and generalise wonderfully.</p>
<p><hr><h1>  sharing-symmetry</h1>How does the symmetry in a dataset/problem get reflected in the functions learned that solve said problem?</p>
<p>Why do we even care about invariance? A definition of a high level representation is could be that the more signals (except for the one we care about) a variable is invariant to, the higher level it is.</p>
<h4>Weight sharing and invariance</h4>
<p>Are there clever ways to share weights so that our networks are invariant to other transforms? Which transforms do we even want to be invariant to?</p>
<p>Given that <a href="https://arxiv.org/abs/1706.01427">relation networks</a> share weights across each relation, what is it we are invariant to?</p>
<h4>A toolkit for different problems</h4>
<p>If we knew the symmetries present in our data, rotational, translational, ?,
and we had different &lsquo;layers&rsquo; that were invariant to these operations we could easily tailor deep networks to our problem.</p>
<h4>Approximate invariance</h4>
<p>Rather that building invariance into our network, eg avg pooled cnns -&gt; local translational invariance, how can we make it easier to learn these invariances from the data?</p>
<h4>Jacobians</h4>
<p>Invariance is invariably a statement about the jacobian of a function. If a function is invariant to x the we know that df/dy = 0. So can we find structure and/or symmetry in the jacobians of our neural networkds that helps us understand what they have learned? Alternatively, can we impose structure on the jacobian to regularise for different invariances?</p>
<h4>Noether&rsquo;s theorem</h4>
<p>Noether&rsquo;s theorem states (roughly and with a few caveats) that if a functional is invariant to some transform then there is some underlying conserved quantity.
Can this logic be applied to weight sharing in neural networks? What does a convolutional neural network conserve?</p>
<p><hr><h1>  creatively-exploring</h1>When we are searching for a plausible solution what are some good heuristics?
A kind of non-local diversity seems like a good idea. But how should these diverse queries be generated? This is what I would define as creativity; the ability to cheaply generate plausible and diverse canidates.</p>
<p>Efficient search comes very close to the explore-exploit tradeoff. Recently we have seen some work achieving efficient exploration through curiosity (and in some sense suprise) guided actions (see <a href="https://pathak22.github.io/noreward-rl/">this</a>). </p>
<p>Given this, an alternative definition of creativity could be the generation of new queries that were considered, a priori, to be inplausiable/not thought of, but in hindsight are effective (aka a good guess? what more is guiding this?)</p>
<p><hr><h1>  reasoning-and-relations</h1>Computers can easily reason about symbols. (With the advent of deep learning) Computers can learn concepts and attach them to symbols. Thus computers can reason about learned concepts. The final step: learn to reason about learned concepts.</p>
<p>If reasoning is the manipulation of learned concepts, then learning to reason would be: learning different operations/functions to compose, reduce, add, intersect, &hellip; learned concepts. See <a href="https://arxiv.org/abs/1707.03389">SCAN</a> for the possible solution.</p>
<p><hr><h1>  resource-constrained-settings</h1>Right now deep learning only works with lots of data, lots of compute, (and lots of patience&hellip;).
Given that we are seeing DL massively scaled across the internet, small wins in efficiency can make a big deal.</p>
<h4>Small compute</h4>
<p>Can we save compute by  </p>
<h4>Small memory</h4>
<p>Online algorithms.</p>
<h4>Small data</h4>
<p>?</p>
<p><hr><h1>  learning-to-learn</h1>If computers are ever going to take over the world, they will need to be able to learn to learn. This kind of self-optimisation, is what I hold dear and I believe that makes us &lsquo;intelligent&rsquo;.
Learning is great, as we are currently finding out, however, there have been a few different learning algorithms over the years. First nature started out with natural selection, but it is incredibly slow O(billions of years&hellip;). Evolution then learned to be more efficient at learning, it invented melleable nervous systems to handle the faster adaptiation needed to survive.</p>
<p>All it really means is understanding a given learning problem in more depth so we can include &lsquo;better&rsquo; inductive biases in the optimiser.</p>
<ul>
<li><a href="https://arxiv.org/abs/1704.03003">Automated Curriculum Learning for Neural Networks</a></li>
<li><a href="https://arxiv.org/pdf/1703.00837.pdf">Meta Networks</a></li>
<li><a href="https://doi.org/10.3200/JMBR.36.3.339-351">Learning to optimize</a></li>
<li><a href="http://arxiv.org/abs/1611.05763">Learning to reinforcement learn</a></li>
<li><a href="http://arxiv.org/abs/1606.04474">Learning to learn by gradient descent by gradient descent</a></li>
<li><a href="http://proceedings.mlr.press/v70/chen17e.htm">Learning to learn without gradient descent by gradient descent</a></li>
</ul>
<p>Remembering memory
Optimising optimisers</p>
<p><hr><h1>  representations-geometry</h1>What is the right geometry for learning?</p>
<!-- Pics of vector fields ![]() -->

<p>Studied in online learning, ?,</p>
<p>Why do cts representations of weight make sense? Why as vectors?</p>
<p>Why does NAT learn better representations than AEs?</p>
<p>Embedding different logics into representations.</p>
<p>How does representation/structure effect function!</p>
<p>Embedding relations, memory, &hellip; into distributed (linear?) representations</p>
<p><hr><h1>  falsifying-hypotheses</h1>
Imagine that we have some oracle (maybe physics for experiment design, a database for query generation, &hellip;) that is expensive to call. Given what we )the learner) know, our next call to the oracle should be to falsify our leading hypothesis. <!-- (about ???) --></p>
<p>Falsification is at the very heart of good science: generate plausible and testable hypotheses, try your hardest to prove them wrong. Keep the remaining hypotheses and repeat. 
In machine learning this process also seems to go by another name, active learning: How can you generate maximally informative queries.</p>
<p><hr><h1>  interpretable-visuals</h1>It is hard to get an intuitive understanding of what has been learned.</p>
<p>Current tools for this are;</p>
<ul>
<li>saliency/propagate gradients back,</li>
<li>?</li>
</ul>
<p>but they are underwhelming.</p>
<p>Better ways to visalise;
<em> computations and their resource use would be good? Visualising the big Ohs?
</em> the learning dynamics
* ?</p>
<p><hr><h1>  big-data</h1>Machine learning is about finding patterns in data. Over the last few years datasets have been rapidly increasing in size, and for good reason. The larger the dataset, the more robust and accurate the learned algorithm. However, as these datasets increase in size we start to encounter new problems. Requiring $$$ for electricity, &hellip;
Making ML only for the rich and powerful. We need &hellip;</p>
<p>Dont want to have to train on multi epochs when the size of the dataset is large.
Want sublinear costs in; memory, compute, data,</p>
<ul>
<li>Mixture of experts</li>
<li>Active learning</li>
</ul>
<p>When you have a lot of data&hellip; Unsupervised/reinforcement!</p>
<p><hr><h1>  universal-approximators</h1>Can we find an representation that approximates arbitrary functions easily?
<!-- Probably not, need to specify a type of problem? -->
Which representations can approximate type functions we care about with low error?</p>
<p>What are the best algorithmic atoms to build out of?
Orthogonal ???s. Polynomials, vectors, graphs, ?,</p>
<p>A neural network is the combination of a few distinct ideas.</p>
<p>*</p>
<p>Why are neural networks better than SVMs, or RBFs, or &hellip;? Are NNs best for every application?</p>
<p>Current wisdom hints that local and heirarchical function approximations are !!, but ?
* The ReLU is discontinuous, is that important?</p>
<p>Why is it that (convolutional) neural networks work so well? Hierarchical, local, discontinuous, ?. </p>
<p><hr><h1>  life-long-learning</h1>
<hr><h1>  functional-loss</h1>Loss functions are usually given to us or tailored for a specific job. Is there some new representation of loss functions that allows;</p>
<h4>Learning the loss function.</h4>
<p>Sometimes it can be hard to write down what we want to optimise, if we could just show some examples of our metric then we could learn that and optimise it? <a href="https://arxiv.org/abs/1406.2661">GANs</a> seem to be an example of this.</p>
<h4>Decomposition.</h4>
<p>Decomposing a loss function into many (<a href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a>). <a href="https://arxiv.org/abs/1608.05343">DNIs</a> seem to do this.
The ability to reduce a complex goal into smaller achievable steps is the goal of XXX.</p>
<p>Doing so would allow;
<em> parallel learning of the different tasks,
</em> specialisation,
*</p>
<p>Also related to <a href="https://arxiv.org/pdf/1703.01161.pdf">Feudal nets</a>.
The important/cool part is having an actual handle to the subproblems.</p>
<h4>First class losses.</h4>
<p>Loss fns as first class fns that can be composed, passed around, traded, &hellip;
Imagine some system, where agents cooperate and compete. Cooperation of competition can be organised by communicating loss functions.</p>
<p>The composition of two loss functions is ?</p>
<p>My goal is A, your goal is B, A and B are similar, let&rsquo;s make babies.
My goal is X, your goal is Y, Y is orthogonal to my X, stay out of my way.</p>
<p><hr><h1>  sketching-models</h1>What if there was a high level language for describing what a NN does/has learned? It could then make sense to communicate this, or to store it in compressed form, or &hellip;?</p>
<p>Given enough trained models, and their respective data. It could make sense to learn a sketch of each!?</p>
<h4>A meta representation of a neural network</h4>
<p>Each &lsquo;network&rsquo; should have;</p>
<ul>
<li>a callable fn f:x-&gt;y (most likely a parameterised network)</li>
<li>a sketch of the inputs and outputs, (x, y) that it was trained on (some sort of generative model).</li>
<li>some belief/knowledge about which labels is it good at. And which data is requires to get better at others.</li>
<li>some descrption/embedding of how it labels a input, the method. What it pays attention to and how it processes it. (this needs to be some sort of graph?)</li>
</ul>
<h4>First class networks</h4>
<p>Can we pass around models as first class functions? </p>
<ul>
<li>What about composing different learned models?</li>
<li>Sketching/compressing a learned model into ???</li>
<li></li>
</ul>
<h4>Generating networks</h4>
<p>If we can represent a network as just some data structre, then it makes sense to be able to generate one.</p>
<p><hr><h1>  identifying-structure</h1>Can we adapt our computations to the structure in our data?
We could adaptively choose the hardware/representations we are using based upon operations we are calculating.</p>
<p>For example: online optimisation of the processing of a NN to make it more efficient. Writing often used processes into hardware.</p>
<p>Structural adaptation of a model based on structure/patterns in the data.</p>
<p>For example;</p>
<ul>
<li>we are learning a linear model. It turns out the linear transform can accurately (within some tolerance, or at least more so that other factorisations) be factorised with a cholesky decompoisition. So replace the linear transform with a cholseky decomposition.</li>
<li>our cloud service is being queried with millions of cosine similarity computations, write X cosine computations into hardware and use them.</li>
</ul>
<!-- Problem this runs into is 'over' specialisation? What if we were wrong? How can we undo the decision? -->

<p>In general, an oracle looks at the operations being done in our network (multi resolution low level &ndash; addition, exp, &hellip; <em>hardware</em> &ndash; and high level &ndash; svd, QP, &hellip; <em>software</em>) and replaces approximations with the closest op. Error of that op could still be tracked.</p>
<p><hr><h1>  trade-offs</h1>There are tradeoffs everywhere in machine learning.
You can trade compute for accuracy (quality of gradient estimates, or even compute second order info), or time and memory (recompute vs remember in BPTT), or XX for sanity&hellip;
For each given application, the requirements will be different. Sometimes memory is more imporant than time, sometimes vice versa.
Ultimately we would like to tune performance given the requirement of the application.</p>
<!-- Could like to my own work on conserved quantities -->

<h4>New resources to trade</h4>
<p>Tradeoff between;
<em> number of parameters and &ldquo;findability&rdquo; of a good configuration.
</em> how much compute do I (the learned algorithm) need to spend to achieve a given level of accuracy.</p>
<h4>Marketplace</h4>
<p>Most interestingly, can we let algorithms find their own optimal tradeoffs, by giving them these tradeoffs as regularisers? Like <a href="https://arxiv.org/abs/1603.08983">Adaptive computation time</a>. 
It would be cool to frame this as some sort of marketplace, where resources are traded according to some currency.</p>
<h4>A conserved quantity?</h4>
<p>Is there some underlying quantity that is being conserved while we tradeoff the resouces? Similar to have gravitational poential energy can be traded for kinetic energy, &hellip; What is traded when we trade accuracy for speed?</p>
<p><hr><h1>  correlated-samples</h1>
<hr><h1>  top-down-attention</h1>
<hr><h1>  curse-of-dimensionality</h1>
<hr></p>
</div>
</body>
</html>