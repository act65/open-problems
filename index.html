<!DOCTYPE html>
<html>
<head>
    <link href="http://netdna.bootstrapcdn.com/twitter-bootstrap/2.3.0/css/bootstrap-combined.min.css" rel="stylesheet">
    <style>
        body {
            font-family: sans-serif;
        }
        code, pre {
            font-family: monospace;
            }
            h1 code,
            h2 code,
            h3 code,
            h4 code,
            h5 code,
            h6 code {
                font-size: inherit;
        }
        div {
            margin-right: 400px;
            margin-left: 400px;
        }
    </style>
</head>
<body>
<div class="container">
<h1>  better-priors</h1>

<p>Progress in learning (especially in unsupervised settings) is likely to be made through our ability to specify domain specific priors. This is because it is not possible to have a universally efficient algorithms.</p>
<ul>
<li>Want a nice easy way, a language, to specify priors.</li>
<li>What different types of prior are there?</li>
</ul>
<p>What we need is better, more flexible and efficient, ways to specify arbitrary priors on our models. <a href="https://arxiv.org/abs/1511.05644">Adversarial autoencoders</a> are an example of this, where we are able to specify priors by example.</p>
<p>Why do we care? Priors allow us to forgo having to learn potentially complicated relationships and take the priors as given. Assumptions to build on. Thus we should expect better efficiency, in data, in compute, in errors, &hellip; assuming our priors are correct. But, what about when our priors are not correct? How will we know?</p>
<p>What domains do we really care about and need to find better priors? How transferable are priors between different settings?</p>
<p>Can be in many forms. Transfer, a model, &hellip; ???</p>
<h2>The &lsquo;right&rsquo; geometry</h2>
<p>(which priors to include in optimisation)</p>
<p>What is the right geometry for learning?</p>
<!-- Pics of vector fields ![]() -->

<p>Studied in online learning, ?,</p>
<p>Why do cts representations of weight make sense? Why as vectors?</p>
<p>Why does NAT learn better representations than AEs?</p>
<p>Embedding different logics into representations.</p>
<p>How does representation/structure effect function!</p>
<p>Embedding relations, memory, &hellip; into distributed (linear?) representations</p>
<h4>Approximators</h4>
<p>(which priors to include in representation)</p>
<p>Can we find an representation that approximates arbitrary functions easily?
Which representations can approximate the types of &lsquo;natural&rsquo; functions we care about?</p>
<p>What are the best algorithmic atoms to build out of?
Orthogonal ???s. Polynomials, vectors, graphs, ?,</p>
<p>A neural network is the combination of a few distinct ideas.</p>
<ul>
<li>Hierarchical layers</li>
<li>Connectionist</li>
<li>Linear parameterisation</li>
</ul>
<p>Why are neural networks better than SVMs, or RBFs, or &hellip;? Are NNs best for every application?</p>
<p>Current wisdom hints that local and hierarchical function approximations are !!, but ?</p>
<ul>
<li>The ReLU is discontinuous, is that important?</li>
</ul>
<p>Why is it that (convolutional) neural networks work so well? Hierarchical, local, discontinuous, ?.</p>
<p><hr><h1>  credit-assignment</h1>Want to assign credit to Jane for her part in NASA sending a rocket to the moon. But how?</p>
<p>Imagine we had an approximate version of backprop. Then, similar to approximation and optimisation we could and generalisation error</p>
<p>Has issues with discrete things.</p>
<p><hr><h1>  curse-of-dimensionality</h1>
<hr><h1>  disentangled-features</h1>In the ideal case we could apply unsupervised learning to a dataset and get back a model of the true generative process.</p>
<p>However, our current best approach, DL, can only learn correlations. So we are left with finding the patterns</p>
<p>Possible to learn more than correlations without interference? Possibly, if we have ??? (Bernhards thingy &ndash; ternary relations).</p>
<p>Want this to be somewhat modeled after grammaticalization and related to compositionality/merge.</p>
<p><hr><h1>  distributed-learning</h1>Fundamentally, learning must be distributed because compute and memory must be distributed. (Computation and memory reduces to physics, and physics tells us there is a universal limit to information density and processing)
Typically, programming languages are designed to abstract us away from this. However, can we be smarter in designing our system to reflect the computations done in learning.</p>
<!-- distributed in space or time or ??  memory in space == memory in time? -->

<h4>Efficient communication</h4>
<p>Since we have distributed the compute, we need to collect and communicate .</p>
<p>Compress data before communicating (but at the lowest level possible)? 
Each node/agent/? At the lowest level, the &lsquo;code&rsquo; would be rather limited? Want to be able to tailor?
??? </p>
<h4>Inhomogenous compute</h4>
<p>Note that if compute is distributed, then data will also need to be distributed.</p>
<ul>
<li><a href="https://arxiv.org/abs/1511.03575">Federated optimisation</a></li>
</ul>
<p>What if;
- inhomogenous compute, how can each node contribute most effectively? (related to multi-agent-systems?)
- low communication bandwidth,
-</p>
<p><hr><h1>  experiment-design</h1>## Falsifying hypotheses</p>
<p>Imagine that we have some oracle (maybe physics for experiment design, a database for query generation, &hellip;) that is expensive to call. Given what we (the learner) know, our next call to the oracle should be to falsify our leading hypothesis. <!-- (about ???) --></p>
<p>Falsification is at the very heart of good science: generate plausible and testable hypotheses, try your hardest to prove them wrong. Keep the remaining hypotheses and repeat.
In machine learning this process also seems to go by another name, <strong>active learning</strong>: How can you generate maximally informative queries.</p>
<h2>As fast as possible</h2>
<p>Queries, q_i, to the oracle take varying amounts of time, t_i, depending on their &lsquo;complexity&rsquo;. Want to learn while minimising total time taken, T. (allow queries to be asked in parallel?)</p>
<p>This setting is motivated by our search for, say, medicine. We want to find a cure to Alzheimers and some experiments take a long time. We want to find the solution as soon as possible.</p>
<h2>Too big to know</h2>
<p>There exists a causal graph &ndash; n vertices and m edges &ndash; and we want to recover its structure. We can observe the values of all vertices, but none of the edges. (this is harder than what we deal with in the real world as we have extra info about edges via knowing the proximity &ndash; in space or sector or &hellip; &ndash; of two vertices.)</p>
<p>There are also some restrictions on the types of query that can be made. Interventions must be local &ndash; an intervention can only control k variables, where k &lt;&lt; n and k &lt; average degree of the vertices.</p>
<p>Measure of accuracy. (Is there an approximate graph isomorphism? That might say G and H are 75% the same but edges a,b are different. Dont need graph isomorphism as we know the nodes are the same. Total number of correct edges/total number of edges&hellip;)</p>
<p>Motivation</p>
<p>Want to capture the idea of how much organisation/energy it takes to do proper experiments/controls in the real world.</p>
<p>Extensions</p>
<p>(what about imperfect/approximate controls??)</p>
<p><hr><h1>  exploring-the-unknown</h1>Extrapolation&hellip;</p>
<h2>Known unknowns</h2>
<p>How do we become aware of ignorance?</p>
<p>It seems like the scientific method (gather data, hypothesise, falsify, reproduce, repeat) is about taking something we know we don&rsquo;t know and attempting to understand it. But how can we optimally draw attention to the things we don&rsquo;t understand, so we can later apply the &lsquo;scientific method&rsquo;.</p>
<p>Some mixture of tracking prediction error and a skeptical view of explanations? Reminds me of (curiosity guided exploration in agent-based settings).
I have vaguely heard of people talk about surprise and attention to details as being good heuristics for spotting ignorance.</p>
<p>First need to identify what we do know. Knowledge about knowledge.</p>
<h2>Creativity</h2>
<p>When we are searching for a plausible solution what are some good heuristics?
A kind of non-local diversity seems like a good idea. But how should these diverse queries be generated? This is what I would define as creativity; the ability to cheaply generate plausible and diverse candidates.</p>
<p>Efficient search comes very close to the explore-exploit tradeoff. Recently we have seen some work achieving efficient exploration through curiosity (and in some sense surprise) guided actions (see <a href="https://pathak22.github.io/noreward-rl/">this</a>).</p>
<p>Given this, an alternative definition of creativity could be the generation of new queries that were considered, a priori, to be implausible/not thought of, but in hindsight are effective (aka a good guess? what more is guiding this?)</p>
<p><hr><h1>  general-intelligence</h1>
<hr><h1>  generalisation</h1>Symbolic versus learned?</p>
<p><hr><h1>  graph-representations</h1>In deep learning all information is represented in finite dimensional vectors spaces/arrays. Can we generalise this to graphs (where an array is just
 a uniform grid graph)?</p>
<p>Why bother? Graphs allow us to represent relations, between nodes. This extra layer of information allows us to represent .</p>
<h4>Composition</h4>
<p>If we have two graphs representing idea <strong>1</strong> and <strong>2</strong>, then how can we efficiently compose these to integrate their knowledge?
Trivially, we could just join where they share similar nodes? But what if we are working with nodes embedded in some real space, where it is non trivial to check similarity? Or, ? what if <strong>1</strong> and <strong>2</strong> are not embedded in the same space, how is it possible to integrate them (it&rsquo;s not?)?</p>
<h4>Compression</h4>
<p>What about compressing a graph? Are there more general connectivity and signal patterns at some lower resoultion? Alternatively, can we generate higher resolution graphs from lower resolution ones? </p>
<p>How is compressing a graph the same/different to clustering it?</p>
<h4>Hierarchical</h4>
<p>What if we could sketch a graph with a fixed length vector? We could imagine a graph where each node contains an embedding of some other graph. </p>
<p>Can we construct hierarchical graph, graphs within graphs.</p>
<h4>Construction</h4>
<p>How can leared systems interact with, output, read from, &hellip; graph structures. Adjacency matrix feels hack. Want to build into computational primitives, need better libraries?</p>
<h4>Graph signal processing</h4>
<p>?</p>
<p><hr><h1>  historical-experiments</h1>In economics &hellip; by chance we see a natural experiment.</p>
<p>The conditions necessary for this are</p>
<p>A counterfactual.</p>
<p>Both A and B start at some &lsquo;similar&rsquo; state, S, but then a single cause, X, acts in only A (but we can assume the cause of A is not linked to any effects). Thus, any effects we see, Y, are caused by X.</p>
<p>Similar in the sense that all the ways A and B are dissimilar are independent of any effects due to X.</p>
<p>For example.</p>
<p>Someone calls a big white feathery thing in the middle of a lake a &lsquo;swan&rsquo;. Ok, so know I know what swans are. Another day, I see a big black feathery thing flying over head and I ask what it is. &lsquo;Oh, that is a swan&rsquo;.</p>
<p>Consider a controlled experiment. We have a population of people, they they are split into two groups, A, B. A receives an intervention and B receives nothing. We make the assumption that A and B differ only by the intervention. That is to say, before the intervention, sim(A, B) ~= 1. Now this isn&rsquo;t true, as different people are participating in A and B. And this hints at a deeper problem with testing causality in reality. It is not possible to create the same environment twice, we cannot run two realities in parallel and see the result of any intervention we make, unless the many-worlds interpretation is correct (unlikely if you ask me).</p>
<!--
We also assume that the act of doing the experiment is independent from any effects we see?!
People knowing they are in an experiment.
People being disrupted from their daily routine to participate in the experiment.

Also assume that the delivery on the control/intervention is independent w.r.t any effects.
 -->

<p>So, what if we could further weaken this measure of similarity? The requirements for A and B to be similar with respect to a given intervention, X, are;
<em> any differences between A and B, defined as C (= A - B) are independent from any effects, Y (= A&rsquo; - B&rsquo; - C = A&rsquo; - A + B&rsquo; - B = dA/dX - dB/dX). (need to include their natural rate of change? dA/dt?)
</em> ?</p>
<p>So in what cases in this could this help?
* Take A and B to be Auckland and Wellington.</p>
<p>Causality still doesn&rsquo;t make sense to me!? F = ma is not a casual statement. It does not say that acceleration of a mass causes force anymore than a force on a mass causes acceleration. What is a social/economic/political version of this? Well educated people in parliament causes good governance is the same thing as good governance by parliament &hellip;???</p>
<p><hr><h1>  identifying-structure</h1>Can we adapt our computations to manipulate structure in our data?
We could adaptively choose the hardware/representations we are using based upon operations we are calculating.</p>
<p>For example: online optimisation of the processing of a NN to make it more efficient. Writing often used processes into hardware.</p>
<p>Structural adaptation of a model based on structure/patterns in the data.</p>
<p>For example;</p>
<ul>
<li>we are learning a linear model. It turns out the linear transform can accurately (within some tolerance, or at least more so that other factorisations) be factorised with a cholesky decomposition. So replace the linear transform with a cholseky decomposition.</li>
<li>our cloud service is being queried with millions of cosine similarity computations, write X cosine computations into hardware and use them.</li>
</ul>
<!-- Problem this runs into is 'over' specialisation? What if we were wrong? How can we undo the decision? -->

<p>In general, an oracle looks at the operations being done in our network (multi resolution low level &ndash; addition, exp, &hellip; <em>hardware</em> &ndash; and high level &ndash; svd, QP, &hellip; <em>software</em>) and replaces approximations with the closest op. Error of that op could still be tracked.</p>
<p><hr><h1>  integrated-knowledge-base</h1>## Leverage existing knowledge</p>
<h2>Life long learning</h2>
<p>Life long learning is hard because as we learn new things, we need to make space for these new things (finite storage).
So we must selectively forget irrelevant details.</p>
<p>In the ideal case we would rework our entire memory to include the new data we have received and attempt to compress this to more efficient remember as much as possible.</p>
<p>Aka online learning? What is the difference?
Not only do we care about a finite memory footprint, but reusing/transferring knowledge from the past.</p>
<p><hr><h1>  interpretable-visuals</h1>It is hard to get an intuitive understanding of what has been learned.</p>
<p>Current tools for this are;</p>
<ul>
<li>saliency/propagate gradients back,</li>
<li>?</li>
</ul>
<p>but they are underwhelming.</p>
<p>Better ways to visalise;
<em> computations and their resource use would be good? Visualising the big Ohs?
</em> the learning dynamics
* ?</p>
<p><hr><h1>  learned-reasoning</h1>Computers can easily reason about symbols. (With the advent of deep learning &ndash; not true&hellip;) Computers can learn concepts and attach them to symbols. Thus computers can reason about learned concepts. The final step: learn to reason about learned concepts.</p>
<p>If reasoning is the manipulation of learned concepts, then learning to reason would be: learning different operations/functions to compose, reduce, add, intersect, &hellip; learned concepts. See <a href="https://arxiv.org/abs/1707.03389">SCAN</a> for the possible solution.</p>
<p><hr><h1>  meta-learning</h1>Aka hyper parameter optimisation?</p>
<h2>Learning to learn</h2>
<p>If computers are ever going to take over the world, they will need to be able to learn to learn. This kind of self-optimisation, is what I hold dear and I believe that makes us &lsquo;intelligent&rsquo;.
Learning is great, as we are currently finding out, however, there have been a few different learning algorithms over the years. First nature started out with natural selection, but it is incredibly slow O(billions of years&hellip;). Evolution then learned to be more efficient at learning, it invented malleable nervous systems to handle the faster adaptation needed to survive.</p>
<p>All it really means is understanding a given learning problem in more depth so we can include &lsquo;better&rsquo; inductive biases in the optimiser.</p>
<ul>
<li><a href="https://arxiv.org/abs/1704.03003">Automated Curriculum Learning for Neural Networks</a></li>
<li><a href="https://arxiv.org/pdf/1703.00837.pdf">Meta Networks</a></li>
<li><a href="https://doi.org/10.3200/JMBR.36.3.339-351">Learning to optimize</a></li>
<li><a href="http://arxiv.org/abs/1611.05763">Learning to reinforcement learn</a></li>
<li><a href="http://arxiv.org/abs/1606.04474">Learning to learn by gradient descent by gradient descent</a></li>
<li><a href="http://proceedings.mlr.press/v70/chen17e.htm">Learning to learn without gradient descent by gradient descent</a></li>
</ul>
<h2>Learning to teach</h2>
<p>Remembering memory
Optimising optimisers</p>
<p><hr><h1>  multi-agent-systems</h1>Collaboration and competition.</p>
<h2>Modeling your opponent</h2>
<p>How smart do you think your opponent is. Lazy.</p>
<p>Why? Greater efficiency can be made by making assumptions (assuming they are right&hellip;). Stereotyping different players behaviors/strategies. Effectively it allows us to make more assumptions and do a better job of modeling the roll out accurately, thus reducing search space.</p>
<h2>Explaining reasoning (teaching another)</h2>
<h2>Receiving reasoning (working with another)</h2>
<p><hr><h1>  real-world</h1>In the real world, data is messy, complicated, &hellip; and it doesn&rsquo;t come how you want it.</p>
<h2>Complexity</h2>
<p>Most real world problems are a little more complicated than differentiating between pictures of cats and dogs.
How can we learn complex functions (while remaining efficient?).</p>
<p>What make a function complex?</p>
<ul>
<li>The inter-relatedness of its input variables? So a function with many independent/separable components is not complex?</li>
<li>the length of dependencies,</li>
<li>the raw amount of possible cases/knowledge/information</li>
<li>?</li>
</ul>
<p>Firstly, we will need to construct learnable classes of complex function, however, this is the problem! To capture many different types of complex function we need &hellip; ??? exponentially large space. Thus complex functions are harder to approximate as there are more candidates to search through. We need more priors to help constrain this&hellip;</p>
<p>There are some approaches to this problem and they largely boil down to something related to Occam&rsquo;s razor. Start simple and build on that.</p>
<ul>
<li>Curriculum, (two types? target and capacity?)</li>
<li>long-term and deep,</li>
<li>Constructing more complex fns from simple ones. Starting with the simplest hypotheses.</li>
</ul>
<h2>Correlated samples</h2>
<p>Pretty much every ML algorithm you come across has an often over looked assumption, that the dataset has been constructed from IID samples from the true data distribution. This is pretty much impossible to achieve in practice and there will always be some bias in the dataset (possibly just from the sampling, but more likely due to how it was constructed).</p>
<p>Game playing (typically with RL) &hellip; this problem as &hellip;?</p>
<p>Another setting where we see this is <a href="https://arxiv.org/abs/1511.03575">Federated optimisation</a>. (but not correlated in time, but in source?)</p>
<p>Related to life-long-learning?</p>
<!-- However, people seem to be good at learning from highly correlated inputs (I actually dont think this is true). I think we use a trick to help us out, we generally have some idea of the space of plausible inputs-outputs and thus we can simulate/imagine the other inputs we are not seeing. -->

<p>When the input distribution is a moving target, samples are correlated in time or ?.</p>
<p>Multi-agent learning
Online learning
Reinforcement learning</p>
<p>Self-play. Only explore subspace (related to diversity)</p>
<h2>Noise</h2>
<p>?</p>
<p><hr><h1>  robots</h1>Humans evolved in a world where we needed to make predictions quickly, given little information and resources. Robots will need to act in this same world.</p>
<!-- Aka problems caused (indirectly) by compute, memory and latency constraints -->

<h4>Aligning delayed signals/rewards</h4>
<p>Image that the vision system might process info within 0.1ms, but the auditory system processed it in 0.5ms. How can we align these two pieces of information, to build a full model of our environment?
And, given that there was a delay in when; we perceived, and acted, received (/perceived) a reward how can we assign credit to act single choice we made in between?</p>
<h4>Shared resources</h4>
<p>Imagine a system that has to learn and predict with the same set of minimal resources.
Limited by max power usage (aka a need for attention!?)
An obvious strategy might be to: predict now, consolidate/learn later? Which seems familiar?</p>
<p>Explore-exploit-consolidate
You have a limited resource; energy supply rate (and/or compute) but you must act in the real world (explore/exploit) while also learning (consolidating).</p>
<h4>Low latency</h4>
<p>Processing complex functions cause a delay in response.
Way around this;</p>
<ul>
<li>to attempt to predict the future and your next step (do the computation early rather than reactively).</li>
<li>A quick guess with slower more accurate corrections?</li>
</ul>
<p>This also allows correction/response to happen faster, as we dont need to perceive what is going on (if we are right), we can just focus on where we predicted wrong and allocate attention/compute there.</p>
<p>Google&rsquo;s solution, offload computation to data centers with TPUs&hellip;</p>
<p><hr><h1>  sample-complexity</h1>Deep learning needs to see the same dataset many times, eventually learns some useful patterns, but not before wandering here and there to checkout what it would be like to do a crappy job.</p>
<h4>One-shot</h4>
<p>We (seem to) have the ability to learn from a single example. How is this possible? Can we get computers to do it?</p>
<p>Trivially, this is just memorisation. The interesting part comes from trying to achieve a sub-linear memory footprint, aka some sort of online compression. And this is where we get back to deep learning. We need to pick some method to compress these examples we are shooting, and deep learning seems like a decent candidate, except for its data complexity.</p>
<p>Most definitions of learning (e.g. PAC) include some guarantee that a single example should not change the output of the learning algorithm by more than epsilon. This seems to preclude the ability to do one-shot learning?</p>
<!-- How do current methods get around this? -->

<!-- Could be closer to memory, decompose new input into things already known, memorise signal over them. Then learning would be the how to decompose. Aka one-shot == memory. -->

<h4>Zero-shot</h4>
<p>If one-shot learning is: &ldquo;Learning from a single example&rdquo; then zero-shot learning would be: &ldquo;making a damn good guess&rdquo;.</p>
<p>Given examples about X, can we learn about Y?</p>
<p>A possible hypothesis for how we manage this is that we hypothesise about causes, we create models/explanations, which are occasionally accurate and generalise wonderfully.</p>
<p><hr><h1>  scaling-deep-learining</h1>Deep learning is really a meta algorithm. Take a set of differentiable layers, compose them together into a hierarchy, use automatic differentiation to efficiently compute gradients w.r.t each layer and finally use stochastic gradient descent to update each layer.
Long story short, it works (as long as you have a lot of labelled data, compute and patience).</p>
<p>So let&rsquo;s apply this approach to as many domains as possible. But, there are functions that are not differentiable;</p>
<ul>
<li>random variables</li>
<li>discrete variables</li>
<li>trees (diff boundary trees), graphs</li>
<li>unknown (ie RL setting)</li>
<li>logic</li>
<li>specialist algorithms</li>
</ul>
<!-- Examples; MuProp, ?

differentiable indexing.
differentiable sparse gating -->

<p>These only make sense in light of adding better priors to make learning cheaper/more interpretable/?</p>
<h2>Big data</h2>
<p>Given that we are seeing DL massively scaled across the internet, small wins in efficiency can make a big deal.</p>
<p>Machine learning is about finding patterns in data. Over the last few years datasets have been rapidly increasing in size, and for good reason. The larger the dataset, the more robust and accurate the learned algorithm. However, as these datasets increase in size we start to encounter new problems. Requiring $$$ for electricity, &hellip;
Making ML only for the rich and powerful. We need &hellip;</p>
<p>Dont want to have to train on multi epochs when the size of the dataset is large.
Want sublinear costs in; memory, compute, data,</p>
<ul>
<li>Mixture of experts</li>
<li>Active learning</li>
</ul>
<p>When you have a lot of data&hellip; Unsupervised/reinforcement!</p>
<p><hr><h1>  sharing-symmetry</h1>How does the symmetry in a dataset/problem get reflected in the functions learned that solve said problem?</p>
<p>Why do we even care about invariance? A definition of a high level representation is could be that the more signals (except for the one we care about) a variable is invariant to, the higher level it is.</p>
<p>The problem is;
<em> one of efficiency? The more symmetry a network has (do CNNs have more symmetry than NNs) the less data required to train it (assuming the symmetry is also represented in that data).
</em> Interpretability.</p>
<h4>Weight sharing and invariance</h4>
<p>Are there clever ways to share weights so that our networks are invariant to other transforms? Which transforms do we even want to be invariant to?</p>
<p>Given that <a href="https://arxiv.org/abs/1706.01427">relation networks</a> share weights across each relation, what is it we are invariant to?</p>
<h4>A toolkit for different problems</h4>
<p>If we knew the symmetries present in our data, rotational, translational, ?,
and we had different &lsquo;layers&rsquo; that were invariant to these operations we could easily tailor deep networks to our problem.</p>
<h4>Approximate invariance</h4>
<p>Rather that building invariance into our network, eg avg pooled cnns across spatial dimensions -&gt; (local) translational invariance, how can we make it easier to learn these invariances from the data?</p>
<h4>Jacobians</h4>
<p>Invariance is a statement about the jacobian of a function. If a function is invariant to x the we know that df/dx = 0. So can we find structure and/or symmetry in the jacobians of our neural networks that helps us understand what they have learned? Alternatively, can we impose structure on the jacobian to regularise for different invariances?</p>
<h4>Noether&rsquo;s theorem</h4>
<p>Noether&rsquo;s theorem states (roughly and with a few caveats) that if a functional is invariant to some transform then there is some underlying conserved quantity.
Can this be applied to weight sharing in neural networks? What does a convolutional neural network conserve?</p>
<p><hr><h1>  sketching-models</h1>What if there was a high level language for describing what a NN does/has learned? It could then make sense to communicate this, or to store it in compressed form, or &hellip;?</p>
<p>Given enough trained models, and their respective data. It could make sense to learn a sketch of each!?</p>
<h4>A meta representation of a neural network</h4>
<p>Each &lsquo;network&rsquo; should have;</p>
<ul>
<li>a callable fn f:x-&gt;y (most likely a parameterised network)</li>
<li>a sketch of the inputs and outputs, (x, y) that it was trained on (some sort of generative model).</li>
<li>some belief/knowledge about which labels is it good at. And which data is requires to get better at others.</li>
<li>some description/embedding of how it labels a input, the method. What it pays attention to and how it processes it. (this needs to be some sort of graph?)</li>
</ul>
<h4>First class networks</h4>
<p>Can we pass around models as first class functions?</p>
<ul>
<li>What about composing different learned models?</li>
<li>Sketching/compressing a learned model into ???
*</li>
</ul>
<h4>Generating networks</h4>
<p>If we can represent a network as just some data structure, then it makes sense to be able to generate one.</p>
<p>This program does X, this program does Y. Can compose them together to do Z. Effectively need a type system&hellip;!</p>
<p><hr><h1>  specifying-goals</h1>Approximating the oracle.
Getting training signal.
Formalising what we want.
Communicating it to a computer.</p>
<h2>Learning by example</h2>
<p>Insert pic of lion.</p>
<p>How do you determine if the image you are looking at is a lion?
Ok, write that process down as a program. Oh&hellip; That&rsquo;s really hard.</p>
<p>If we can write what we want down as a clear function/equation, then we can probably optimise it. If not. Hmm.</p>
<p>Machine learning allows an alternative approach to specifying what we want. It allows us to &lsquo;show&rsquo; the computer what we want by example. Pairs for images and their labels, &hellip;</p>
<p>It is not always possible to use examples to show the computer what we want, for examples learning p(x). GANs? Allow us to extend this &hellip;
<!-- What about generators of goals? (that RL stuff?) --></p>
<p>In RL our goal is some game state/set of states. It seems reasonable to be able to compress this set of game states into some sort of hidden representation that we can compare against. Or communicate to others.
<!-- But if you set the goal as a game state, then how does improvement make sense? Once the state has been achieved there is nothing left to do...? --></p>
<p>However. What about when the goal is some process or function or algorithm? How can we represent sets of these? By their input/output? &hellip;??</p>
<!-- This is closely related to learning loss functions! -->

<p>Different ways to specify a goal/function:</p>
<!-- What do we mean by specify?
- Choose,
- narrow down,
-

so it is a way to reduce search space.
what about falsification?
-->

<ul>
<li>can show by example, (but what if we only know the goal, and not how that relates to inputs? that is just RL, as opposed to SL, where we have pairs.)</li>
<li>have a function, f, that tells us (true, false, &hellip;) whether y is the goal (where f may or may not be differentiable). UL or SL.</li>
<li>Reducing possible options. eg. can only pick from some set of functions (e.g. linear, &hellip;).</li>
<li>randomly&hellip;</li>
</ul>
<p>What about if we dont know what we want? Then we must either;</p>
<ul>
<li>specify a process for finding new things,</li>
<li>simulate all possible goals and then pick out the ones we like.</li>
</ul>
<p>What if we could specify goals using natural language.</p>
<p>What about the efficient specification of goals? Examples take space, equations do not.</p>
<h4>Learning the loss function.</h4>
<p>Sometimes it can be hard to write down what we want to optimise, if we could just show some examples of our metric then we could learn that and optimise it? <a href="https://arxiv.org/abs/1406.2661">GANs</a> seem to be an example of this.</p>
<h2>Label complexity</h2>
<p>Supervised learning requires annotated data!</p>
<p>Possible (half) solution?
A cheaper way to annotate data. Hierarchical label structures? Interactive exploration of data (that is also fun)?</p>
<p>New and clever ways of extracting annotations from existing datasets. Something like the how unsupervised methods extract labels from the existing data. Train on context, recontruction, ?, real/fake, &hellip;</p>
<p>Projects that satisfy these wants;
<em> Interactive tensorboard visualisations. Ability to add meta data while viewing the <a href="http://projector.tensorflow.org/">projector</a>.
</em> ?</p>
<p>Loss functions are usually given to us or tailored for a specific job. Is there some new representation of loss functions that allows;</p>
<h4>Decomposition.</h4>
<p>Decomposing a loss function into many (<a href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a>). <a href="https://arxiv.org/abs/1608.05343">DNIs</a> seem to do this.
The ability to reduce a complex goal into smaller achievable steps is the goal of XXX.</p>
<p>Doing so would allow;
<em> parallel learning of the different tasks,
</em> specialisation,
*</p>
<p>Also related to <a href="https://arxiv.org/pdf/1703.01161.pdf">Feudal nets</a>.
The important/cool part is having an actual handle to the subproblems.</p>
<h4>First class losses.</h4>
<p>A type of algebra that makes it easy to specify goals.</p>
<p>Loss fns as first class fns that can be composed, passed around, traded, &hellip;
Imagine some system, where agents cooperate and compete. Cooperation of competition can be organised by communicating loss functions.</p>
<p>The composition of two loss functions is ?</p>
<p>My goal is A, your goal is B, A and B are similar, let&rsquo;s make babies.
My goal is X, your goal is Y, Y is orthogonal to my X, stay out of my way.</p>
<p><hr><h1>  trade-offs</h1>There are tradeoffs everywhere in machine learning.
You can trade compute for accuracy (quality of gradient estimates, or even compute second order info), or time and memory (recompute vs remember in BPTT), or XX for sanity&hellip;
For each given application, the requirements will be different. Sometimes memory is more imporant than time, sometimes vice versa.
Ultimately we would like to tune performance given the requirement of the application.</p>
<!-- Could like to my own work on conserved quantities -->

<h4>New resources to trade</h4>
<p>Tradeoff between;
<em> number of parameters and &ldquo;findability&rdquo; of a good configuration.
</em> how much compute do I (the learned algorithm) need to spend to achieve a given level of accuracy.</p>
<h4>Marketplace</h4>
<p>Most interestingly, can we let algorithms find their own optimal tradeoffs, by giving them these tradeoffs as regularisers? Like <a href="https://arxiv.org/abs/1603.08983">Adaptive computation time</a>.
It would be cool to frame this as some sort of marketplace, where resources are traded according to some currency.</p>
<h4>A conserved quantity?</h4>
<p>Is there some underlying quantity that is being conserved while we tradeoff the resouces? Similar to have gravitational poential energy can be traded for kinetic energy, &hellip; What is traded when we trade accuracy for speed?</p>
<hr>
</div>
</body>
</html>